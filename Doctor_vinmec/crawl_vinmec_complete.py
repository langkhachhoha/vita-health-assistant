import requests
from bs4 import BeautifulSoup
import json
import re

def crawl_multiple_doctors():
    """
    Script crawl nhiá»u bÃ¡c sÄ© Vinmec tá»« danh sÃ¡ch URLs
    """
    
    print("ğŸš€ Báº®T Äáº¦U CRAWL NHIá»€U BÃC SÄ¨ VINMEC")
    print("=" * 60)
    
    # Load danh sÃ¡ch URLs tá»« file Ä‘Ã£ thu tháº­p
    try:
        with open('doctor_urls_list.json', 'r', encoding='utf-8') as f:
            urls_data = json.load(f)
        doctor_urls = urls_data.get('doctor_urls', [])
        print(f"ğŸ“Š ÄÃ£ load {len(doctor_urls)} URLs tá»« file doctor_urls_list.json")
        
        # Giá»›i háº¡n sá»‘ lÆ°á»£ng bÃ¡c sÄ© Ä‘á»ƒ test (cÃ³ thá»ƒ thay Ä‘á»•i)
        max_doctors = 10  # Chá»‰ crawl 10 bÃ¡c sÄ© Ä‘áº§u tiÃªn Ä‘á»ƒ test
        if len(doctor_urls) > max_doctors:
            doctor_urls = doctor_urls[:max_doctors]
            print(f"ğŸ”„ Giá»›i háº¡n crawl {max_doctors} bÃ¡c sÄ© Ä‘áº§u tiÃªn Ä‘á»ƒ test")
            
    except FileNotFoundError:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file doctor_urls_list.json, sá»­ dá»¥ng URLs máº·c Ä‘á»‹nh")
        doctor_urls = [
            "https://www.vinmec.com/vie/chuyen-gia-y-te/pham-nhat-an-50932-vi",
            "https://www.vinmec.com/vie/chuyen-gia-y-te/do-tat-cuong-416-vi"
        ]
    except Exception as e:
        print(f"âŒ Lá»—i khi Ä‘á»c file URLs: {e}")
        return None
    
    all_doctors = []
    successful_crawls = 0
    
    for i, url in enumerate(doctor_urls, 1):
        print(f"\nğŸ“‹ ÄANG CRAWL BÃC SÄ¨ {i}/{len(doctor_urls)}")
        print(f"ğŸ”— URL: {url}")
        print("-" * 50)
        
        try:
            doctor_info = crawl_single_doctor(url)
            if doctor_info and doctor_info.get('ten_bac_si'):
                all_doctors.append(doctor_info)
                successful_crawls += 1
                print(f"âœ… ThÃ nh cÃ´ng: {doctor_info['ten_bac_si']}")
            else:
                print("âŒ KhÃ´ng trÃ­ch xuáº¥t Ä‘Æ°á»£c thÃ´ng tin")
                
        except Exception as e:
            print(f"âŒ Lá»—i khi crawl: {e}")
        
        # ThÃªm delay giá»¯a cÃ¡c request
        if i < len(doctor_urls):
            print("â³ Chá» 2 giÃ¢y trÆ°á»›c khi crawl tiáº¿p...")
            import time
            time.sleep(5)
    
    # LÆ°u táº¥t cáº£ thÃ´ng tin bÃ¡c sÄ©
    if all_doctors:
        output_file = 'vinmec_doctors_database.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_doctors, f, ensure_ascii=False, indent=2)
        
        print("\n" + "=" * 60)
        print("ğŸ“Š Tá»”NG Káº¾T:")
        print(f"âœ… Crawl thÃ nh cÃ´ng: {successful_crawls}/{len(doctor_urls)} bÃ¡c sÄ©")
        print(f"ğŸ’¾ ÄÃ£ lÆ°u vÃ o file: {output_file}")
        
        # In danh sÃ¡ch bÃ¡c sÄ©
        print("\nğŸ‘¨â€âš•ï¸ DANH SÃCH BÃC SÄ¨:")
        for i, doctor in enumerate(all_doctors, 1):
            print(f"{i}. {doctor.get('ten_bac_si', 'N/A')} - {doctor.get('noi_lam_viec', 'N/A')}")
        
        print("=" * 60)
        print("ğŸ‰ HOÃ€N THÃ€NH Táº¤T Cáº¢!")
        
        return all_doctors
    else:
        print("\nâŒ KhÃ´ng crawl Ä‘Æ°á»£c bÃ¡c sÄ© nÃ o!")
        return None

def crawl_single_doctor(url):
    """
    Crawl thÃ´ng tin má»™t bÃ¡c sÄ© tá»« URL
    """
    
    try:
        # Headers Ä‘á»ƒ giáº£ láº­p trÃ¬nh duyá»‡t
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        
        # Gá»­i request
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # Parse HTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # TrÃ­ch xuáº¥t thÃ´ng tin bÃ¡c sÄ©
        doctor_info = extract_doctor_info(soup, url)
        
        return doctor_info
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ Lá»—i khi táº£i trang: {e}")
        return None
    except Exception as e:
        print(f"âŒ Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh: {e}")
        return None

def crawl_vinmec_doctor_profile():
    """
    Script cÅ© Ä‘á»ƒ crawl má»™t bÃ¡c sÄ© (giá»¯ láº¡i Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch)
    """
    url = "https://www.vinmec.com/vie/chuyen-gia-y-te/pham-nhat-an-50932-vi"
    return crawl_single_doctor(url)

def extract_doctor_info(soup, url=""):
    """
    TrÃ­ch xuáº¥t thÃ´ng tin chi tiáº¿t tá»« HTML
    """
    doctor_info = {
        "url": url,
        "ten_bac_si": "",
        "anh_dai_dien": "",
        "gioi_thieu": "",
        "chuyen_mon": [],
        "noi_lam_viec": "",
        "dich_vu": [],
        "dao_tao": [],
        "kinh_nghiem_lam_viec": []
    }
    
    try:
        print("ğŸ” Äang tÃ¬m section profile_doctor...")
        
        # TÃ¬m section profile_doctor
        profile_section = soup.find('section', class_='profile_doctor')
        if not profile_section:
            print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y section profile_doctor")
            return doctor_info
        
        print("âœ… TÃ¬m tháº¥y section profile_doctor")
        
        # TrÃ­ch xuáº¥t tá»« col-5 (thÃ´ng tin cÆ¡ báº£n)
        col_5 = profile_section.find("div", class_="col-5")
        if col_5:
            print("ğŸ” Äang trÃ­ch xuáº¥t thÃ´ng tin tá»« col-5...")
            extract_basic_info(col_5, doctor_info, soup)
        
        # TrÃ­ch xuáº¥t tá»« col-7 (chi tiáº¿t)
        col_7 = profile_section.find("div", class_="col-7")
        if col_7:
            print("ğŸ” Äang trÃ­ch xuáº¥t thÃ´ng tin tá»« col-7...")
            extract_detailed_info(col_7, doctor_info)
        
    except Exception as e:
        print(f"âŒ Lá»—i khi trÃ­ch xuáº¥t thÃ´ng tin: {e}")
    
    return doctor_info

def extract_basic_info(col_5, doctor_info, soup):
    """
    TrÃ­ch xuáº¥t thÃ´ng tin cÆ¡ báº£n tá»« col-5
    """
    try:
        # 1. TÃŠN BÃC SÄ¨
        name_element = col_5.find('div', class_='f22 bold cl-blue mt1 mb1')
        if name_element:
            doctor_info["ten_bac_si"] = clean_text(name_element.get_text())
            print(f"   âœ… TÃªn: {doctor_info['ten_bac_si']}")
        
        # 2. áº¢NH Äáº I DIá»†N
        avar_doctor_div = col_5.find('div', class_='flex avar_doctor')
        if avar_doctor_div:
            img_link = avar_doctor_div.find('a', class_='thumbblock thumb200x255')
            if img_link:
                img_element = img_link.find('img')
                if img_element and img_element.get('src'):
                    img_src = img_element.get('src')
                    if img_src.startswith('/'):
                        doctor_info["anh_dai_dien"] = f"https://www.vinmec.com{img_src}"
                    else:
                        doctor_info["anh_dai_dien"] = img_src
                    print(f"   âœ… áº¢nh: {doctor_info['anh_dai_dien']}")
        
        # 3. GIá»šI THIá»†U Äáº¦Y Äá»¦
        desc_detail = col_5.find("div", class_="desc_detail")
        if desc_detail:
            paragraphs = desc_detail.find_all("p")
            full_intro = []
            for p in paragraphs:
                text = clean_text(p.get_text())
                if text:
                    full_intro.append(text)
            
            if full_intro:
                doctor_info["gioi_thieu"] = "\n\n".join(full_intro)
                print(f"   âœ… Giá»›i thiá»‡u: {len(doctor_info['gioi_thieu'])} kÃ½ tá»±")
        
    except Exception as e:
        print(f"   âŒ Lá»—i khi trÃ­ch xuáº¥t thÃ´ng tin cÆ¡ báº£n: {e}")

def extract_detailed_info(col_7, doctor_info):
    """
    TrÃ­ch xuáº¥t thÃ´ng tin chi tiáº¿t tá»« col-7
    """
    try:
        # 1. CHUYÃŠN MÃ”N (positions)
        positions_section = col_7.find("div", {"id": "positions"})
        if positions_section:
            position_divs = positions_section.find_all('div', class_='mt1')
            for div in position_divs:
                text = clean_text(div.get_text())
                if text:
                    doctor_info["chuyen_mon"].append(text)
            print(f"   âœ… ChuyÃªn mÃ´n: {len(doctor_info['chuyen_mon'])} má»¥c")
        
        # 2. NÆ I LÃ€M VIá»†C (hospitals)
        hospitals_section = col_7.find("div", {"id": "hospitals"})
        if hospitals_section:
            workplace_link = hospitals_section.find('a')
            if workplace_link:
                doctor_info["noi_lam_viec"] = clean_text(workplace_link.get_text())
                print(f"   âœ… NÆ¡i lÃ m viá»‡c: {doctor_info['noi_lam_viec']}")
        
        # 3. Dá»ŠCH Vá»¤ (services)
        services_section = col_7.find("div", {"id": "services"})
        if services_section:
            service_list = services_section.find('ul', class_='list_dot')
            if service_list:
                service_items = service_list.find_all('li')
                for li in service_items:
                    text = clean_text(li.get_text())
                    if text:
                        doctor_info["dich_vu"].append(text)
            print(f"   âœ… Dá»‹ch vá»¥: {len(doctor_info['dich_vu'])} má»¥c")
        
        # 4. ÄÃ€O Táº O (educations)
        educations_section = col_7.find("div", {"id": "educations"})
        if educations_section:
            content_div = educations_section.find('div', class_='content')
            if content_div:
                education_paragraphs = content_div.find_all('p')
                for p in education_paragraphs:
                    text = clean_text(p.get_text())
                    if text and text != "...":
                        doctor_info["dao_tao"].append(text)
            print(f"   âœ… ÄÃ o táº¡o: {len(doctor_info['dao_tao'])} má»¥c")
        
        # 5. KINH NGHIá»†M LÃ€M VIá»†C (experience)
        work_exp_section = col_7.find("div", {"id": "experience"})
        if work_exp_section:
            content_div = work_exp_section.find('div', class_='content')
            if content_div:
                work_paragraphs = content_div.find_all('p')
                for p in work_paragraphs:
                    text = clean_text(p.get_text())
                    if text and text != "...":
                        doctor_info["kinh_nghiem_lam_viec"].append(text)
            print(f"   âœ… Kinh nghiá»‡m: {len(doctor_info['kinh_nghiem_lam_viec'])} má»¥c")
        
    except Exception as e:
        print(f"   âŒ Lá»—i khi trÃ­ch xuáº¥t thÃ´ng tin chi tiáº¿t: {e}")

def clean_text(text):
    """
    LÃ m sáº¡ch text: loáº¡i bá» khoáº£ng tráº¯ng thá»«a, kÃ½ tá»± Ä‘áº·c biá»‡t
    """
    if not text:
        return ""
    
    # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
    text = re.sub(r'\s+', ' ', text.strip())
    
    # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t khÃ´ng mong muá»‘n
    text = text.replace('\xa0', ' ')  # Non-breaking space
    text = text.replace('\u200b', '')  # Zero-width space
    
    return text

if __name__ == "__main__":
    # Cháº¡y script crawl nhiá»u bÃ¡c sÄ©
    print("ğŸ¤– CHá»ŒN CHá»¨C NÄ‚NG:")
    print("1. Crawl nhiá»u bÃ¡c sÄ© (máº·c Ä‘á»‹nh)")
    print("2. Crawl má»™t bÃ¡c sÄ©")
    
    # Máº·c Ä‘á»‹nh crawl nhiá»u bÃ¡c sÄ©
    result = crawl_multiple_doctors()
    
    if result:
        print(f"\nğŸ‰ ÄÃ£ crawl thÃ nh cÃ´ng {len(result)} bÃ¡c sÄ©!")
    else:
        print("\nğŸ’¥ Script gáº·p lá»—i!")
