import requests
from bs4 import BeautifulSoup
import json
import time
import re

def collect_doctor_urls():
    """
    Thu tháº­p táº¥t cáº£ URLs cá»§a bÃ¡c sÄ© tá»« trang chuyÃªn gia y táº¿ Vinmec
    Tá»« trang 2 Ä‘áº¿n trang 10
    """
    
    print("ğŸš€ Báº®T Äáº¦U THU THáº¬P DANH SÃCH URL BÃC SÄ¨ VINMEC")
    print("=" * 60)
    
    base_url = "https://www.vinmec.com/vie/chuyen-gia-y-te"
    all_doctor_urls = []
    
    # Headers Ä‘á»ƒ giáº£ láº­p trÃ¬nh duyá»‡t
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'vi-VN,vi;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
    }
    # Thu tháº­p tá»« trang 2 Ä‘áº¿n trang 20
    for page_num in range(2, 21):  # 2 Ä‘áº¿n 20
        if page_num == 2:
            url = f"{base_url}/page_2"
        else:
            url = f"{base_url}/page_{page_num}"
        
        print(f"\nğŸ“„ ÄANG THU THáº¬P TRANG {page_num}")
        print(f"ğŸ”— URL: {url}")
        print("-" * 40)
        
        try:
            # Gá»­i request
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            print("âœ… Táº£i trang thÃ nh cÃ´ng!")
            
            # Parse HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # TÃ¬m táº¥t cáº£ links bÃ¡c sÄ©
            doctor_links = extract_doctor_links(soup)
            
            if doctor_links:
                print(f"ğŸ” TÃ¬m tháº¥y {len(doctor_links)} bÃ¡c sÄ©")
                all_doctor_urls.extend(doctor_links)
                
                # In má»™t vÃ i URL máº«u
                for i, link in enumerate(doctor_links[:3], 1):
                    print(f"   {i}. {link}")
                if len(doctor_links) > 3:
                    print(f"   ... vÃ  {len(doctor_links) - 3} bÃ¡c sÄ© khÃ¡c")
            else:
                print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y bÃ¡c sÄ© nÃ o")
            
            # Delay giá»¯a cÃ¡c trang
            if page_num < 100:
                print("â³ Chá» 3 giÃ¢y trÆ°á»›c khi táº£i trang tiáº¿p...")
                time.sleep(3)
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ Lá»—i khi táº£i trang {page_num}: {e}")
            continue
        except Exception as e:
            print(f"âŒ Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh trang {page_num}: {e}")
            continue
    
    # Loáº¡i bá» URL trÃ¹ng láº·p
    unique_urls = list(set(all_doctor_urls))
    
    # LÆ°u káº¿t quáº£
    if unique_urls:
        # Sáº¯p xáº¿p URLs
        unique_urls.sort()
        
        # LÆ°u vÃ o JSON
        output_data = {
            "total_doctors": len(unique_urls),
            "collected_from_pages": list(range(2, 11)),
            "doctor_urls": unique_urls
        }
        
        with open('doctor_urls_list.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        # LÆ°u vÃ o file text Ä‘Æ¡n giáº£n
        with open('doctor_urls.txt', 'w', encoding='utf-8') as f:
            for url in unique_urls:
                f.write(url + '\n')
        
        print("\n" + "=" * 60)
        print("ğŸ“Š Tá»”NG Káº¾T THU THáº¬P:")
        print(f"ğŸ“„ ÄÃ£ kiá»ƒm tra: {9} trang (trang 2-10)")
        print(f"ğŸ‘¨â€âš•ï¸ Tá»•ng sá»‘ bÃ¡c sÄ©: {len(unique_urls)} URLs")
        print(f"ğŸ’¾ ÄÃ£ lÆ°u vÃ o: doctor_urls_list.json")
        print(f"ğŸ“ ÄÃ£ lÆ°u vÃ o: doctor_urls.txt")
        
        # Hiá»ƒn thá»‹ má»™t vÃ i URL máº«u
        print(f"\nğŸ”— MáºªU URLS (10 Ä‘áº§u tiÃªn):")
        for i, url in enumerate(unique_urls[:10], 1):
            doctor_name = extract_doctor_name_from_url(url)
            print(f"{i:2d}. {doctor_name} - {url}")
        
        if len(unique_urls) > 10:
            print(f"    ... vÃ  {len(unique_urls) - 10} bÃ¡c sÄ© khÃ¡c")
        
        print("=" * 60)
        print("ğŸ‰ HOÃ€N THÃ€NH THU THáº¬P URLS!")
        
        return unique_urls
    
    else:
        print("\nâŒ KhÃ´ng thu tháº­p Ä‘Æ°á»£c URL nÃ o!")
        return None

def extract_doctor_links(soup):
    """
    TrÃ­ch xuáº¥t táº¥t cáº£ links bÃ¡c sÄ© tá»« má»™t trang
    """
    doctor_links = []
    
    try:
        # TÃ¬m táº¥t cáº£ cÃ¡c link cÃ³ href chá»©a '/vie/chuyen-gia-y-te/' vÃ  cÃ³ class 'list_name_doctor'
        # Dá»±a trÃªn structure HTML báº¡n cung cáº¥p
        
        # CÃ¡ch 1: TÃ¬m trong cÃ¡c tháº» a cÃ³ class chá»©a 'list_name_doctor'
        doctor_link_elements = soup.find_all('a', class_=lambda x: x and 'list_name_doctor' in x)
        
        for link in doctor_link_elements:
            href = link.get('href')
            if href and '/vie/chuyen-gia-y-te/' in href:
                # Táº¡o URL Ä‘áº§y Ä‘á»§ náº¿u cáº§n
                if href.startswith('/'):
                    full_url = f"https://www.vinmec.com{href}"
                else:
                    full_url = href
                doctor_links.append(full_url)
        
        # CÃ¡ch 2: TÃ¬m táº¥t cáº£ links chá»©a pattern '/vie/chuyen-gia-y-te/'
        if not doctor_links:
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link.get('href')
                if href and '/vie/chuyen-gia-y-te/' in href and href.count('-') >= 2:  # URL bÃ¡c sÄ© thÆ°á»ng cÃ³ dáº¡ng name-id-vi
                    if href.startswith('/'):
                        full_url = f"https://www.vinmec.com{href}"
                    else:
                        full_url = href
                    doctor_links.append(full_url)
        
        # CÃ¡ch 3: TÃ¬m trong div cÃ³ class chá»©a 'flex' vÃ  'doctor'
        if not doctor_links:
            flex_divs = soup.find_all('div', class_=lambda x: x and 'flex' in x)
            for div in flex_divs:
                links = div.find_all('a', href=True)
                for link in links:
                    href = link.get('href')
                    if href and '/vie/chuyen-gia-y-te/' in href:
                        if href.startswith('/'):
                            full_url = f"https://www.vinmec.com{href}"
                        else:
                            full_url = href
                        doctor_links.append(full_url)
        
    except Exception as e:
        print(f"   âŒ Lá»—i khi trÃ­ch xuáº¥t links: {e}")
    
    # Loáº¡i bá» duplicates trong trang nÃ y
    return list(set(doctor_links))

def extract_doctor_name_from_url(url):
    """
    TrÃ­ch xuáº¥t tÃªn bÃ¡c sÄ© tá»« URL
    """
    try:
        # URL cÃ³ dáº¡ng: /vie/chuyen-gia-y-te/pham-nhat-an-50932-vi
        parts = url.split('/')
        if len(parts) > 0:
            name_part = parts[-1]  # Láº¥y pháº§n cuá»‘i
            # Loáº¡i bá» sá»‘ vÃ  -vi
            name_clean = re.sub(r'-\d+-vi$', '', name_part)
            # Thay tháº¿ - báº±ng space vÃ  title case
            name_formatted = name_clean.replace('-', ' ').title()
            return name_formatted
    except:
        pass
    return "Unknown Doctor"

def create_url_list_for_crawler():
    """
    Táº¡o danh sÃ¡ch URL Ä‘á»ƒ sá»­ dá»¥ng cho crawler chÃ­nh
    """
    try:
        # Äá»c file URLs Ä‘Ã£ thu tháº­p
        with open('doctor_urls_list.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        urls = data.get('doctor_urls', [])
        
        if urls:
            print(f"\nğŸ”§ Táº O FILE Cáº¤U HÃŒNH CHO CRAWLER CHÃNH")
            print(f"ğŸ“Š CÃ³ {len(urls)} URLs Ä‘á»ƒ crawl")
            
            # Táº¡o file Python config
            config_content = f'''# Auto-generated URL list for Vinmec doctor crawler
# Generated from {len(data.get("collected_from_pages", []))} pages
# Total doctors found: {len(urls)}

DOCTOR_URLS = [
'''
            
            for url in urls[:50]:  # Giá»›i háº¡n 50 URLs Ä‘áº§u tiÃªn Ä‘á»ƒ test
                config_content += f'    "{url}",\n'
            
            config_content += ''']

# Usage example:
# from doctor_urls_config import DOCTOR_URLS
# for url in DOCTOR_URLS:
#     crawl_single_doctor(url)
'''
            
            with open('doctor_urls_config.py', 'w', encoding='utf-8') as f:
                f.write(config_content)
            
            print("âœ… ÄÃ£ táº¡o file: doctor_urls_config.py")
            print(f"ğŸ“ Chá»©a {min(50, len(urls))} URLs Ä‘áº§u tiÃªn")
            
        else:
            print("âŒ KhÃ´ng cÃ³ URLs Ä‘á»ƒ táº¡o config")
            
    except Exception as e:
        print(f"âŒ Lá»—i khi táº¡o config: {e}")

if __name__ == "__main__":
    # Thu tháº­p URLs
    urls = collect_doctor_urls()
    
    if urls:
        # Táº¡o file config cho crawler chÃ­nh
        create_url_list_for_crawler()
        
        print(f"\nğŸ¯ BÆ¯á»šC TIáº¾P THEO:")
        print("1. Kiá»ƒm tra file doctor_urls_list.json")
        print("2. Sá»­ dá»¥ng URLs nÃ y cho crawler chÃ­nh")
        print("3. Hoáº·c import tá»« doctor_urls_config.py")
    else:
        print("\nğŸ’¥ KhÃ´ng thu tháº­p Ä‘Æ°á»£c URLs!")
